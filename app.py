# ==============================================================================
# 1. IMPORTACIONES Y CONFIGURACI√ìN INICIAL
# ==============================================================================
import streamlit as st
import os
import google.generativeai as genai

# --- Importaciones de LangChain ---
from langchain.agents import AgentExecutor, create_tool_calling_agent, create_react_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain.memory import ConversationBufferMemory
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
from langchain import hub # Para el agente ReAct

# ##############################################################################
# Se utiliza st.secrets para leer las claves de API de forma segura desde
# la configuraci√≥n de Streamlit Cloud. Este m√©todo es el recomendado.
# ##############################################################################
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    HUGGINGFACEHUB_API_TOKEN = st.secrets["HUGGINGFACEHUB_API_TOKEN"]
    TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]

    # Configurar las variables de entorno para que LangChain las detecte autom√°ticamente
    # Aunque ya las tenemos en variables, muchas librer√≠as buscan directamente en os.environ
    os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN
    os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY

except KeyError as e:
    st.error(f"Error: No se encontr√≥ el secreto '{e.args[0]}'.")
    st.error("Por favor, aseg√∫rate de haber configurado todas las claves (GOOGLE_API_KEY, HUGGINGFACEHUB_API_TOKEN, TAVILY_API_KEY) en los 'Secrets' de tu app en Streamlit Cloud.")
    st.stop() # Detiene la ejecuci√≥n si falta alguna clave

# ==============================================================================
# 2. DEFINICI√ìN DE HERRAMIENTAS (HABILIDADES DEL AGENTE)
# ==============================================================================

@tool
def web_search(query: str) -> str:
    """Busca en la web informaci√≥n actualizada, incluyendo fuentes de datos para investigaci√≥n acad√©mica."""
    try:
        # La clase buscar√° la API key en las variables de entorno si no se pasa expl√≠citamente
        search = TavilySearchAPIWrapper()
        return search.run(query)
    except Exception as e:
        return f"Error en la b√∫squeda web: {e}"

@tool
def summarize_paper(pdf_path: str) -> str:
    """Carga y resume un art√≠culo de investigaci√≥n en formato PDF. Extrae la metodolog√≠a, resultados y conclusiones clave."""
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load_and_split()
        
        full_text = " ".join([page.page_content for page in pages])
        
        # Usar un LLM espec√≠fico y r√°pido para la tarea de resumen
        summarizer_llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.2)
        
        prompt_template = f"""
        Basado en el siguiente texto de un paper acad√©mico, por favor, crea un resumen conciso y estructurado (aproximadamente 300 palabras).
        El resumen debe enfocarse en:
        1. **Problema y Objetivos:** ¬øQu√© pregunta busca responder el paper?
        2. **Metodolog√≠a:** ¬øQu√© m√©todos usa (ej. modelo de panel, DSGE, SVAR)? ¬øCu√°l es la fuente de datos?
        3. **Hallazgos Clave:** ¬øCu√°les son los resultados m√°s importantes y significativos?
        4. **Conclusiones:** ¬øCu√°l es la implicaci√≥n principal del estudio?

        Texto del Paper:
        {full_text[:25000]} 
        """
        
        summary = summarizer_llm.invoke(prompt_template).content
        return summary
    except Exception as e:
        return f"Error al procesar el PDF: {e}"

tools = [web_search, summarize_paper]

# ==============================================================================
# 3. CONFIGURACI√ìN DEL AGENTE Y LA MEMORIA
# ==============================================================================

# Prompt para el agente Tool Calling (Gemini)
tool_calling_prompt = ChatPromptTemplate.from_messages([
    ("system", """Eres un asistente de investigaci√≥n de doctorado de clase mundial.
    Tu misi√≥n es ayudar al usuario a avanzar en su tesis sobre la transici√≥n energ√©tica, precios al carbono y modelado econ√≥mico.
    - Usa tus herramientas cuando sea necesario para buscar informaci√≥n o analizar documentos.
    - Cuando te pidan c√≥digo para modelos (Panel Data, SVAR, DSGE, Streamlit), genera el c√≥digo Python directamente.
    - Siempre responde de forma rigurosa, clara y acad√©mica."""),
    ("placeholder", "{chat_history}"),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])

# Prompt para el agente ReAct (Hugging Face)
react_prompt = hub.pull("hwchase17/react")

# Inicializar la memoria para el historial del chat
if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ==============================================================================
# 4. INTERFAZ DE USUARIO CON STREAMLIT
# ==============================================================================

st.set_page_config(page_title="Asistente de Tesis IA", layout="wide")
st.title("ü§ñ Asistente de Tesis IA para Transici√≥n Energ√©tica")

# La verificaci√≥n de claves ahora est√° al principio, as√≠ que podemos quitarla de aqu√≠.

# --- Barra Lateral para Controles ---
with st.sidebar:
    st.header("Configuraci√≥n")
    model_choice = st.selectbox(
        "Elige tu modelo:",
        ("Google Gemini-1.5-Pro", "Mistral-7B (via Hugging Face)")
    )
    temperature = st.slider(
        "Temperatura (creatividad):", 
        min_value=0.0, max_value=1.0, value=0.4, step=0.1
    )
    
    uploaded_file = st.file_uploader("Sube un paper (PDF)", type="pdf")
    
    if uploaded_file:
        temp_dir = "temp_pdf"
        if not os.path.exists(temp_dir):
            os.makedirs(temp_dir)
        
        # Guardamos el archivo temporalmente
        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        st.session_state.uploaded_file_path = temp_file_path
        st.success(f"Archivo '{uploaded_file.name}' cargado y listo para analizar.")

# --- L√≥gica de Selecci√≥n de Modelo y Construcci√≥n de Agente ---
# ##############################################################################
# ### CAMBIO 2: CORRECCI√ìN MENOR EN LA L√ìGICA DEL MODELO ###
# Se ajusta la condici√≥n para que coincida exactamente con el texto del selectbox.
# ##############################################################################
if model_choice == "Google Gemini-1.5-Pro":
    st.info("Nota: Se usar√° el modelo 'gemini-1.5-flash' para optimizar la velocidad y la cuota gratuita.")
    # SOLUCI√ìN 1: Usamos gemini-1.5-flash-latest, es m√°s r√°pido y tiene una cuota m√°s generosa.
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=temperature, convert_system_message_to_human=True)
    agent = create_tool_calling_agent(llm, tools, tool_calling_prompt)

else: # "Mistral-7B (via Hugging Face)"
    st.info("Nota: Se usar√° el modelo 'Mixtral-8x7B' ya que Mistral-7B no est√° en la capa gratuita de la API.")
    # SOLUCI√ìN 2: Usamos un modelo m√°s potente que S√ç est√° disponible en la capa gratuita.
    # El modelo Mixtral es una excelente alternativa.
    repo_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"
    llm = HuggingFaceEndpoint(repo_id=repo_id, temperature=temperature)
    agent = create_react_agent(llm, tools, react_prompt)


agent_executor = AgentExecutor(
    agent=agent, 
    tools=tools, 
    memory=st.session_state.memory, 
    verbose=True, 
    handle_parsing_errors=True # Muy √∫til para agentes ReAct
)

# --- L√≥gica del Chat ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Hola, soy tu asistente de investigaci√≥n. ¬øC√≥mo puedo ayudarte hoy con tu tesis?"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_prompt := st.chat_input("Pregunta sobre papers, datos, modelos..."):
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    with st.chat_message("assistant"):
        input_for_agent = {"input": user_prompt}
        
        # A√±adir contexto del archivo subido si existe
        if 'uploaded_file_path' in st.session_state and st.session_state.uploaded_file_path:
            input_for_agent["input"] += (
                f"\n\n[Contexto Adicional] El usuario ha subido el archivo ubicado en: "
                f"'{st.session_state.uploaded_file_path}'. "
                f"Si la pregunta se refiere a 'el paper' o 'el documento', usa la herramienta `summarize_paper` con esa ruta."
            )

        with st.spinner("Procesando tu solicitud..."):
            response = agent_executor.invoke(input_for_agent)
            st.markdown(response["output"])
        
    st.session_state.messages.append({"role": "assistant", "content": response["output"]})

    # Limpiar el archivo despu√©s de su uso para la siguiente interacci√≥n
    if 'uploaded_file_path' in st.session_state and st.session_state.uploaded_file_path:
        if os.path.exists(st.session_state.uploaded_file_path):
            os.remove(st.session_state.uploaded_file_path)
        del st.session_state.uploaded_file_path

# ==============================================================================
# 1. IMPORTACIONES Y CONFIGURACIN INICIAL
# ==============================================================================
import streamlit as st
import os
import google.generativeai as genai
import nest_asyncio
import asyncio

# Aplicar el parche para resolver conflictos de bucles de eventos as铆ncronos
nest_asyncio.apply()

# --- Importaciones de LangChain ---
from langchain.agents import AgentExecutor, create_tool_calling_agent, create_react_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain.memory import ConversationBufferMemory
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
from langchain import hub

# ==============================================================================
# 2. CARGA DE SECRETS Y VARIABLES DE ENTORNO
# ==============================================================================
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    HUGGINGFACEHUB_API_TOKEN = st.secrets["HUGGINGFACEHUB_API_TOKEN"]
    TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]

    os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN
    os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY

except KeyError as e:
    st.error(f"Error: No se encontr贸 el secreto '{e.args[0]}'.")
    st.error("Aseg煤rate de haber configurado todas las claves en los 'Secrets' de tu app.")
    st.stop()

# ==============================================================================
# 3. DEFINICIN DE HERRAMIENTAS
# ==============================================================================
@tool
def web_search(query: str) -> str:
    """Busca en la web informaci贸n actualizada."""
    try:
        search = TavilySearchAPIWrapper()
        return search.run(query)
    except Exception as e:
        return f"Error en la b煤squeda web: {e}"

@tool
def summarize_paper(pdf_path: str) -> str:
    """Carga y resume un art铆culo de investigaci贸n en formato PDF."""
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load_and_split()
        full_text = " ".join([page.page_content for page in pages])
        summarizer_llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.2)
        prompt_template = f"Basado en el siguiente texto de un paper, crea un resumen conciso (aprox. 300 palabras) enfoc谩ndote en: Problema, Metodolog铆a, Hallazgos y Conclusiones.\n\nTexto:\n{full_text[:25000]}"
        summary = summarizer_llm.invoke(prompt_template).content
        return summary
    except Exception as e:
        return f"Error al procesar el PDF: {e}"

tools = [web_search, summarize_paper]

# ==============================================================================
# 4. CONFIGURACIN DEL AGENTE Y LA MEMORIA
# ==============================================================================
tool_calling_prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente de investigaci贸n de doctorado. Tu misi贸n es ayudar al usuario a avanzar en su tesis. Usa tus herramientas. Responde de forma rigurosa y acad茅mica."),
    ("placeholder", "{chat_history}"),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])
react_prompt = hub.pull("hwchase17/react")
if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ==============================================================================
# 5. INTERFAZ DE USUARIO Y LGICA PRINCIPAL
# ==============================================================================
st.set_page_config(page_title="Asistente de Tesis IA", layout="wide")
st.title(" Asistente de Tesis IA")

with st.sidebar:
    st.header("Configuraci贸n")
    model_choice = st.selectbox("Elige tu modelo:", ("Google Gemini-1.5-Flash", "Hugging Face (Flan-T5)"))
    temperature = st.slider("Temperatura (creatividad):", 0.0, 1.0, 0.4, 0.1)
    
    uploaded_file = st.file_uploader("Sube un paper (PDF)", type="pdf")
    if uploaded_file:
        temp_dir = "temp_pdf"
        if not os.path.exists(temp_dir): os.makedirs(temp_dir)
        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_file_path, "wb") as f: f.write(uploaded_file.getbuffer())
        st.session_state.uploaded_file_path = temp_file_path
        st.success(f"Archivo '{uploaded_file.name}' cargado.")

# --- L贸gica de Selecci贸n de Modelo ---
if model_choice == "Google Gemini-1.5-Flash":
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=temperature, convert_system_message_to_human=True)
    agent = create_tool_calling_agent(llm, tools, tool_calling_prompt)
else:
    llm = HuggingFaceEndpoint(repo_id="google/flan-t5-xxl", temperature=0.1, max_new_tokens=1024)
    agent = create_react_agent(llm, tools, react_prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools, memory=st.session_state.memory, verbose=True, handle_parsing_errors="Check your output and try again.")

# --- L贸gica del Chat ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Hola, soy tu asistente. 驴C贸mo te ayudo?"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]): st.markdown(message["content"])

# --- Funci贸n auxiliar as铆ncrona (SOLO PARA GEMINI) ---
async def get_gemini_response(agent_executor, input_dict):
    return await agent_executor.ainvoke(input_dict)

if user_prompt := st.chat_input("Pregunta sobre papers, datos, modelos..."):
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"): st.markdown(user_prompt)

    with st.chat_message("assistant"):
        input_for_agent = {"input": user_prompt}
        if 'uploaded_file_path' in st.session_state and st.session_state.uploaded_file_path:
            input_for_agent["input"] += f"\n\n[Contexto] El usuario ha subido el archivo: '{st.session_state.uploaded_file_path}'. Usa `summarize_paper` si es relevante."

        with st.spinner("Procesando..."):
            # --- CAMBIO CLAVE FINAL: Usamos el m茅todo de llamada correcto para cada agente ---
            if model_choice == "Google Gemini-1.5-Flash":
                # Para Gemini, usamos el m茅todo as铆ncrono moderno y eficiente.
                response = asyncio.run(get_gemini_response(agent_executor, input_for_agent))
            else:
                # Para el agente ReAct de HF, usamos el m茅todo s铆ncrono cl谩sico y robusto.
                # `nest_asyncio` nos protege de los conflictos.
                response = agent_executor.invoke(input_for_agent)
            
            st.markdown(response["output"])
        
    st.session_state.messages.append({"role": "assistant", "content": response["output"]})

    if 'uploaded_file_path' in st.session_state and st.session_state.uploaded_file_path:
        if os.path.exists(st.session_state.uploaded_file_path): os.remove(st.session_state.uploaded_file_path)
        del st.session_state.uploaded_file_path
